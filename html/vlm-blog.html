<!DOCTYPE html>
<html>
<head>
	<title>VLM</title>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<meta name="format-detection" content="telephone=no">
	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="author" content="">
	<meta name="keywords" content="">
	<meta name="description" content="">
	<link href="https://cdn.bootcdn.net/ajax/libs/twitter-bootstrap/5.3.0-alpha3/css/bootstrap.min.css" rel="stylesheet">
	<link id="theme" rel="stylesheet" type="text/css" href="../css/greyson-theme.css">
	<link rel="stylesheet" type="text/css" href="../style.css">
	<link rel="stylesheet" type="text/css" href="../fonts/icomoon/icomoon.css">
	
	<link rel="preconnect" href="https://fonts.googleapis.com">
	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link href="https://fonts.googleapis.com/css2?family=Mulish:ital@0;1&family=Oswald:wght@500&display=swap" rel="stylesheet">

	<meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>VLM</title>
    <meta name="keywords" content="Corporate Business Shopse HTML5 Css3 Template App Product">
    <meta name="description" content="appnox - Product Landing HTML5 Template">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" type="image/x-icon" href="../assets/img/favicon.ico">

    <!-- css here -->
    <link rel="stylesheet" href="../static/css/bootstrap.min.css">
    <link rel="stylesheet" href="../static/css/meanmenu.css">
    <link rel="stylesheet" href="../static/css/animate.min.css">
    <link rel="stylesheet" href="../static/css/magnific-popup.css">
    <link rel="stylesheet" href="../static/css/fontawesome-all.min.css">
    <link rel="stylesheet" href="../static/css/owl.carousel.min.css">
    <link rel="stylesheet" href="../static/css/owl.theme.default.min.css">
    <link rel="stylesheet" href="../static/css/scrolltop.css">
    <link rel="stylesheet" href="../static/css/default.css">
    <link rel="stylesheet" href="../static/css/style.css">
    <link rel="stylesheet" href="../static/css/responsive.css">
</head>
<body>

<div class="pattern-bg"></div>

<header id="header">
	<nav class="navbar navbar-expand-lg navbar-light">
		<div class="container-fluid">
		  <a class="navbar-brand" href="../index.html"><img src="../images/logo.png" alt="logo"></a>
		  <button class="navbar-toggler" type="button" data-bs-toggle="offcanvas" data-bs-target="#offcanvasNavbar"
			aria-controls="offcanvasNavbar">
			<span class="navbar-toggler-icon"></span>
		  </button>
		  <div class="offcanvas offcanvas-end" tabindex="-1" id="offcanvasNavbar"
			aria-labelledby="offcanvasNavbarLabel">
			<div class="offcanvas-header">
			  <h5 class="offcanvas-title" id="offcanvasNavbarLabel">Offcanvas</h5>
			  <button type="button" class="btn-close text-reset" data-bs-dismiss="offcanvas" aria-label="Close"></button>
			</div>
			<div class="offcanvas-body">
			  <ul class="navbar-nav justify-content-end fw-bold fs-6 text-uppercase flex-grow-1 pe-3 gap-3">
				<li class="nav-item">
				  <a class="nav-link active" aria-current="page" href="../index.html">Home</a>
				</li>
				<li class="nav-item">
				  <a class="nav-link" href="#">Freebies</a>
				</li>
				<li class="nav-item dropdown">
				  <a class="nav-link dropdown-toggle" href="#" id="dropdownId" data-bs-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Pages</a>
				  <div class="dropdown-menu" aria-labelledby="dropdownId">
					<a class="dropdown-item" href="../index.html">Blog</a>
					<a class="dropdown-item" href="../index.html">Blog Single</a>
				  </div>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#">Blog</a>
				</li>
				<li class="nav-item">
					<a class="nav-link" href="#">Contact</a>
				</li>
			  </ul>
			</div>
		  </div>
		</div>
	</nav>
</header>

<main>
<div id="appnox-latest-blog-area">
	<div class="container">
		<div class="row">
			<div class="blog-details-wraper">
				 <div class="blog-details-content">

                            <h2 style="font-weight: bold;text-align: center;color: black"> VLM 大模型整合 </h2>
                            <p style="font-size: 20px;color: darkgrey;text-align: center">2023年12月4日</p>
                            <p style="color: black">以下内容是根据<a href="https://mmbench.opencompass.org.cn/leaderboard">MMBench
                                (opencompass.org.cn)</a>的multi-modality榜单进行相应的整理，截止时间为2023年12月4日，具体每个模型的介绍参考了GitHub，技术报告，以及目前网上公众号的相应介绍。
                            </p>
                            <!--                            <img src="static/picture/details-1.jpg" alt="">-->
                            <ul>
                                <li><a href="#section1">1 - InternLM-XComposer - 2023.9.26</a></li>
                                <li><a href="#section2">2 - TransCore-M - 2023.11.30</a></li>
                                <li><a href="#section3">3 - Sphinx-V2 - </a></li>
                                <li><a href="#section4">4 - LLaVA-v1.5-13B - 2023.10.05</a></li>
                                <li><a href="#section5">5 - mPLUG-Owl2 - 2023.11.10</a></li>
                                <li><a href="#section6">6 - GPT-4V - 2023.9.25</a></li>
                                <li><a href="#section7">7 - CogVLM - 2023.10.27</a></li>
                                <li><a href="#section8">8 - MMICL - 2023.8.18</a></li>
                                <li><a href="#section9">9 - LMEye - 2023.10.16</a></li>
                                <li><a href="#section10">10 - Qwen-VL-Chat - 2023.8.24</a></li>
                            </ul>

                            <h3 id="section1" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://github.com/InternLM/InternLM-XComposer">1. InternLM-XComposer-VL</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: <a href="https://huggingface.co/spaces/Willow123/InternLM-XComposer">InternLM XComposer </a></li>
                                <li style="list-style-type: circle;padding-left: 0px">Time: 2023.9.26</li>
                                <li style="list-style-type: circle">Company: Shanghai Artificial Intelligence Laboratory
                                </li>
                                <li style="list-style-type: circle">Parameters: 8B</li>
                                <li style="list-style-type: circle">Language Model: InternLM-7B</li>
                                <li style="list-style-type: circle">Vision Model: EVA-G</li>
                                <li style="list-style-type: circle">GitHub:
                                    https://github.com/InternLM/InternLM-XComposer
                                </li>
                                <li style="list-style-type: circle">Paper: https://arxiv.org/abs/2309.15112</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/owGkaTkq1ScEAG8kH58nBA
                                </li>
                            </ul>
                            <h5>1.1 简介</h5>
                            <p>浦语·灵笔模型是基于书生·浦语大语言模型研发的<b>视觉-语言大模型</b>，提供图文理解和创作能力：</p>
                            <ul>
                                <li style="list-style-type: disc;margin-left: 20px"><b>图文交错创作:</b>
                                    浦语·灵笔可以为用户打造图文并貌的文章，具体是提供<b>文章生成和配图选择</b>的功能。这一能力由以下步骤实现：
                                </li>
                                <li style="list-style-type: circle;margin-left: 40px">理解用户指令，创作符合要求的文章</li>
                                <li style="list-style-type: circle;margin-left: 40px">智能分析文章，自动规划插图的理想位置，确定图像内容需求。</li>
                                <li style="list-style-type: circle;margin-left: 40px">基于以文搜图服务，从图库中检索出对应图片。</li>
                                <li style="list-style-type: disc;margin-left: 20px"><b>图文理解: </b>浦语·灵笔设计了高效的训练策略，为模型注入海量的多模态概念和知识数据，赋予其强大的图文理解和对话能力。
                                </li>
                            </ul>
                            <p>从公布的技术报告可以获悉InternLM-XComposer在公开评测数据集上的战绩：在多项视觉语言大模型的主流评测上均取得了最佳性能，包括MME
                                Benchmark (英文评测)、 MMBench (英文评测)、Seed-Bench (英文评测)、 CCBench(中文评测)、MMBench-CN (中文评测)。
                            </p>
                            <p>截至目前(2023年10月14日)官方开源2个版本的浦语·灵笔模型:</p>
                            <ul>
                                <li style="list-style-type: disc;margin-left: 20px">该模型是基于书生·浦语大语言模型的多模态预训练和多任务训练模型，在多种评测上表现出杰出性能, 例如：MME Benchmark, MMBench Seed-Bench, CCBench, MMBench-CN。该模型是base模型，即常说的基座模型。</li>
                                <li style="list-style-type: disc;margin-left: 20px">进一步对InternLM-XComposer-VL进行指令微调得到nternLM-XComposer。该微调模型可以用于创作图文并茂的文章，也支持多模态对话(目前支持图文对话，更具体是围绕给定图片的讨论，暂不支持指令编辑给定的图片)。</li>
                            </ul>
                            <h5>1.2 模型架构</h5>
                            <p>InternLM-XComposer整体架构由3部分组成，包括视觉编码器、LLM-model (本文即InternLM) 和对齐模块(报告中称为Perceive Sampler，感知采样器)。
                            </p>
                            <div style="text-align: center"><img src="../images\VLM\1-1.png" alt="" style="zoom:80%"></div>
                            <ul>
                                <li style="list-style-type: disc">
                                    <b>视觉编码器：</b>InternLMXComposer 中的视觉编码器采用 EVA-CLIP，这是标准 CLIP的改进版本，用mask图像的方式增强了模型的建模能力，能够更有效地捕捉输入图像的视觉细节。在该模块中，图像被调整为统一的 224×224 尺寸，然后以步长为14的方式切割成图块。这些图块作为输入token，再利用transformer中的自注意力机制，从而提取图像embeddings。
                                </li>
                                <li style="list-style-type: disc">
                                    <b>Perceive 采样器：</b>InternLM-XComposer中的感知采样器其实是一种池化机制，旨在将图像embeddings从初始的257维度压缩为64。这些压缩优化后的embeddings随后与大型语言模型理解的知识进行对齐。仿照BLIP2的做法，InternLM-XComposer利用BERT-base中cross-attention层作为感知采样器。
                                </li>
                                <li style="list-style-type: disc">
                                    <b>大型语言模型：</b>InternLM-XComposer用InternLM 作为其基础大型语言模型。值得注意的是，InternLM 是一种功能强大的多语言模型，擅长英语和中文。具体是使用已经公开的 InternLM-Chat-7B 作为大型语言模型。
                                </li>
                            </ul>
                            <h5>1.3 Training</h5>
                            <p>InternLM-XComposer的训练过程分为 A 阶段和 B 阶段。A 阶段作为预训练阶段，利用大量数据训练基座模型。B阶段是监督微调，先做多任务训练，再做指令微调。在多任务训练后得到InternLM-XComposer-VL模型，在指令微调得到InternLM-XComposer模型。</p>
                            <p><b>（1） Pre-training阶段</b>：预训练基座视觉语言模型阶段利用了大量的图像-文本对(网络爬取)和交错的图像-文本数据。这些数据包括中英文两种语言的多模态数据。为了保持大型语言模型的语言能力，在 InternLM <b>预训练阶段</b>使用的<b>部分文本数据</b>也在 InternLM-XComposer 的<b>预训练阶段</b>中使用。如Table 1 所示：</p>
                            <div style="text-align: center"><img src="../images\VLM\1-2.png" alt="" style="zoom:80%"></div>
                            <p>图像-文本对方面，使用了 11 亿张图像和 677 亿个文本token(其中506亿个英文文本token和171亿个中文文本token)，包括公开数据集和从内部数据(从网站爬取收集)，
                                总共超过 1100 万个语义概念。内部数据集In-house Data有一个开放的子集：书生·万卷文本数据集。
                                这个开源数据集，包含三个部分：纯文本格式数据集、文本-图像对数据集和视频数据集，可以从官网下载到，
                                相关的数据说明也可以在对应论文上查阅。此外，还加入了约100亿个从InternLM 预训练数据集中抽样的文本token，
                                以<b>维持模型的语言能力</b>。在训练过程中，所有预训练数据都经过了严格的清洗流程，以确保其质量和可靠性。</p>
                            <p>在预训练阶段，视觉编码器参数固定，主要对感知采样器和大型语言模型进行优化。
                                感知采样器和大型语言模型的<b>初始权重</b>分别来自BLIP2和InternLM。
                                由于大型语言模型天然缺乏对图像embeddings的理解，因此在多模态预训练框架内做优化有助于提高理解这些embeddings的能力。
                                模型的训练目标集中在下一个token预测上，使用交叉熵损失作为损失函数。
                                采用的优化算法是 AdamW，超参数设置如下：β1=0.9，β2=0.95，eps=1e-8。
                                感知采样器和大型语言模型的最大学习速率分别设置为 2e-4 和 4e-5，采用余弦学习率衰减策略，
                                最小学习率设置为1e-5。此外，在最初的 200 步中使用线性预热。训练过程一个batch size中约 1570 万个token，
                                并进行 8000 次迭代。使用如此大的batch size有助于稳定训练，同时也有助于维持InternLM的固有能力。</p>
                            <p><b>（2） 监督微调阶段：</b>在预训练阶段，图像embeddings与文本表征对齐，使大型语言模型具有初步具备理解图像内容的能力。
                                为进一步指导模型在恰当时机使用图像信息的能力，引入了各种视觉-语言任务。
                                所以，整个监督微调阶段，其实由多任务训练和指令微调组成。</p>
                            <p><b>多任务训练</b>。多任务训练数据集如Table 2 所示，这些任务包括场景理解（例如 COCO Caption，SUB）、位置理解（例如 Visual Spatial Reasoning 数据集）、
                                光学字符识别OCR（例如 OCR-VQA）以及开放式回答（例如 VQAv2 ，GQA）等。</p>
                            <div style="text-align: center"><img src="../images\VLM\1-3.png" alt="" style="zoom:80%"></div>
                            <p>每个任务都被设计成会话交互形式，具体格式如下：</p>
                            <div style="text-align: center"><img src="../images\VLM\1-4.png" alt="" style="zoom:67%"></div>
                            <p>其中&lt;eou&gt;和&lt;eob&gt; 分别表示用户和机器人结束token。对于每张图像具有多个问题的QVA数据集，将其构造成多轮对话，
                                问题随机排序，从而大大提高了 SFT 过程的效率。在这个阶段，所有问题都通过人工编写的提示引入，以增加任务的多样性。
                                为了实现稳定且高效的微调，将大型语言模型的权重冻结，然后使用 Low-Rank Adaption（LoRA）架构进行模型微调。
                                当然，在这个过程中，感知采样器也同时进行训练，只是学习率不同。LoRA 应用于注意力层的 query、value 和 key 以及前馈网络。
                                实验发现<b>较高的LoRA rank有助于赋予模型新能力</b>；因此，将 LoRA rank和 alpha 参数都设置为 256。
                                模型在10,000次迭代训练中使用全局batch size，值为256。LoRA 层和感知采样器的学习速率分别设置为 5e-5和 2e-5。</p>
                            <p><b>指令微调</b>。为了进一步提升上述模型的指令跟随和交错的图像-文本组合能力，使用来自纯文本对话语料库和LLava-150k 的数据进行指令微调。此外，使用 LRV 数据集减轻幻觉。交错的图像-文本组合数据集的构建方法，可以查阅原始技术报告，这里略过。Batch size=256，并在1000 次迭代中使用较小的学习速率=1e-5。</p>
                            <h5>1.4 Application</h5>
                            <div style="text-align: center"><img src="../images\VLM\1-5.png" alt="" style="zoom:80%"></div>

                            <h3 id="section2" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://github.com/PCIResearch/TransCore-M">2. TransCore-M</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: <a href="http://123.249.36.167:82/?wework_cfm_code=OUGvQ%2BJaUrnG4qWwQQ3TCHPRNka5YXBQWGQa%2FTC2bCAm8PKw9d%2F6EwBCvtiSYljSsWs6t1Odmh%2FemGunAy8KKx4%3D">PCI_Research</a>
                                <li style="list-style-type: circle">Time: 2023.11.30</li>
                                <li style="list-style-type: circle">Company: PCI Research</li>
                                <li style="list-style-type: circle">Parameters:  13.4B</li>
                                <li style="list-style-type: circle">Language Model: PCITransGPT-13B</li>
                                <li style="list-style-type: circle">Vision Model: CLIP ViT/L-14</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/PCIResearch/TransCore-M
                                </li>
                                <li style="list-style-type: circle">Paper:  None</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/2O9UaRIxBotKrX3mTYij1g
                                </li>
                            </ul>
                            <h5>2.1 模型架构与训练</h5>
                            <div style="text-align: center"><img src="../images\VLM\2-1.png" alt="" style="zoom:80%"></div>
                            <p>TransCore-M 采用预训练和指令微调两种训练策略，来提升模型的多模态能力：</p>
                            <ul>
                                <li style="list-style-type: disc">
冻结视觉模块（Visual Encoder）和语言模型（PCITransGPT），使用大量图文数据将图像和文本知识进行对齐；
                                </li>
                                <li style="list-style-type: disc">
                                    构造多样性的文本和多模态联合数据，保持视觉模块冻结，将视觉对齐模块和语言模型进行全参微调，使得模型能够具备更丰富的多模态理解能力。
                                </li>
                            </ul>
                            <h3 id="section3" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/main/SPHINX">3. Sphinx-V2</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: <a href="http://imagebind-llm.opengvlab.com/">imagebind-llm.opengvlab.com</a>
                                <li style="list-style-type: circle">Time: 2023.11.15</li>
                                <li style="list-style-type: circle">Company: Shanghai AI Laboratory</li>
                                <li style="list-style-type: circle">Parameters:  16.5B</li>
                                <li style="list-style-type: circle">Language Model: LLaMA2 13B</li>
                                <li style="list-style-type: circle">Vision Model: CLIP ViT-L/14 CLIP ConvNeXT ConvNeXT-XXLarge Dinov2-VIT-G/14</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/Alpha-VLLM/LLaMA2-Accessory/tree/main/SPHINX
                                </li>
                                <li style="list-style-type: circle">Paper: https://github.com/Alpha-VLLM/LLaMA2-Accessory/blob/main/SPHINX/SPHINX_paper.pdf</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/tNpfmxgS3oNhyWLKzBxq2g
                                </li>
                            </ul>
                            <h5>3.1 简介</h5>
                            <p>本文作者提出了 SPHINX，其是一个通用的多模态大型语言模型（LMM），它同时结合了多任务混合、多 embedding 混合和多权重混合技术。</p>
                            <ul>
                                <li style="list-style-type: disc">
                                    首先，为了更强的视觉-语言对齐，在预训练期间冻结 LLM，并融合分别基于真实世界数据和合成数据训练的 LLM 的权重，通过直接集成两个领域的权重，混合 LLM 可以有效地集成不同的语义，具有良好的鲁棒性。
                                </li>
                                <li style="list-style-type: disc">
                                    其次，为了实现多用途，作者还混合了各种任务进行联合视觉指令微调，并设计了特定任务的指令，以避免任务间的冲突。除了基本的视觉问答任务外，作者还增加了更具有挑战性的任务，比如区域级理解（region-level understanding）、定位描述（caption grounding）、文档布局检测（document layout detection）和人体姿态估计（human pose estimation），有助于不同场景中相互增强。
                                </li>
                                <li style="list-style-type: disc">
                                    此外，还提出了基于各种网络结构、预训练范式和信息粒度的模型提取全面的视觉 embedding，为语言模型提供了更强大的图像表示。
                                </li>
                            </ul>
                            <p>基于提出的联合混合方案，SPHINX 在广泛的应用中表现出卓越的多模态理解能力。在此基础上，作者提出了一种有效的策略，旨在更好地捕捉高分辨率图像的细粒度信息。通过混合不同比例和高分辨率子图像，SPHINX 在现有的评估基准上获得了卓越的视觉理解和推理性能。</p>
                            <h5>3.2 模型架构</h5>
                            <h6>3.2.1 模型权重、调优任务和视觉嵌入的联合混合</h6>
                            SPHINX 的整体混合模式如下图 Figure 3 所示，包括：
                            <div style="text-align: center"><img src="../images\VLM\3-1.png" alt="" style="zoom:80%"></div>
                            <ul>
                                <li style="list-style-type: disc">
                                    <b>Task Mix.</b> For all-purpose capabilities, we mix a variety of vision-language tasks for mutual improvement: VQA, REC, REG, OCR, DET, POSE, REL DET, T2I, etc.
                                </li>
                                <li style="list-style-type: disc">
                                    <b>Embedding Mix.</b> We capture robust visual representations by fusing distinct visual architectures, pre-training, and granularity.
                                </li>
                                <li style="list-style-type: disc">
                                    <b>Domain Mix.</b> For data from real-world and synthetic domains, we mix the weights of two domain-specific models for complementarity.
                                </li>
                            </ul>
                            <p>整个训练分为两阶段：第一阶段是视觉-语言对齐的预训练阶段，第二阶段是视觉指令遵循学习的微调阶段。在这两个阶段中，分别应用模型权重和微调任务的混合。该模型由一个 LLM（例如 LLaMA-2）、混合的视觉编码器（mixing of vision encoders）以及两个线性投影层组成。</p>
                            <p style="font-weight: bold">（1）预训练（Unfreezing LLM for stage-1 pre-training）</p>
                            <p>当前很多 LMM 会在预训练期间冻结 LLM，使用图像-文本描述数据训练中间投影层实现视觉-语言对齐。这种策略可以防止 LLM 过度拟合到只生成短句，因为预训练的描述数据大多包含对图像的简短描述。然而，冻结权重在很大程度上限制了具有大规模视觉-语言数据的 LLM 的跨模态学习潜力。因此，作者建议不冻结 LLM，以实现更充分的视觉、语言对齐。</p>
                            <p>此外，也有工作冻结视觉编码器以获得高质量的图像表示。为了保留 LLM 的长句生成能力，作者使用额外的文本语料数据补充了现有的视觉语言数据用于纯 LLM 微调。更具体地说，在每次迭代中，从语言和视觉语言数据集中采样一个文本和几个图像-描述数据。</p>
                            <p style="font-weight: bold">（2）混合模型权重Weight Mix（Mixed model weights of different domains）</p>
                            <p>来自特定领域的视觉语言数据可能包含不同的语义知识，例如 LAION-COCO 的合成描述、以及 LAION-400M 的真实世界描述。作者提出了一种将不同领域微调模型融合的策略，以集成来自真实世界和合成数据的各自知识。</p>
                            <ul>
                                <li style="list-style-type: disc">利用真实世界的领域数据（比如 LAION-400M）进行预训练，以赋予 LMM 基本的视觉理解能力。</li>
                                <li style="list-style-type: disc">将得到的预训练模型作为合成域（LAION-COCO）上进一步微调 LLM 的预训练模型。</li>
                                <li style="list-style-type: disc">为了利用最佳数据域，直接将两个 LLM 的权重进行加权混合，以进行语义聚合。</li>
                            </ul>
                            <p>提出的权重融合策略可以鼓励每个 LLM 更好地学习领域独特的知识，并对任何新的数据领域表现出灵活的可扩展性。如上图（3）Domian mix部分。</p>
                            <p style="font-weight: bold">（3）混合任务微调（Mixed tuning tasks for stage-2 fine-tuning）</p>
                            <p>经过预训练和模型权重混合，LMM 在视觉和语言数据之间实现了令人满意的对齐，为了进一步提高指令遵循能力，作者从广泛的多模态任务中收集指令数据，并共同微调模型，以学习通用视觉能力，而不是特定领域专家。针对不同的任务都会使用不同的指令。</p>
                            <p style="font-weight: bold">（4）混合视觉编码器嵌入（Mixed embeddings for visual encoding）</p>
                            <p>如Embedding Mix所示，视觉编码器混合了 4 个模型，分别为 CLIP-ViT、CLIP-ConvNeXt、DINOv2-ViT 和 BLIP2 Q-Former。然后使用两个投影层 Proj1 和 Proj2 实现特征投影。</p>
                            <ul>
                                <li style="list-style-type: disc">模型结构不同：CNN 和 ViT 聚合不同类型的视觉特征，即相邻依赖关系和远程交互。</li>
                                <li style="list-style-type: disc">不同的预训练范式：监督训练可以从文本描述或类别标签中加强显式语义信息，而自监督（DINO v2）则强制模型探索隐式信息。</li>
                                <li style="list-style-type: disc">信息粒度不同：CLIP-ViT、CLIP-ConvNeXt、DINOv2-ViT 都在 Patch 级别产生视觉 Token，BLIP2 Q-Former 通过 Query 从全局上下文中总结视觉 embedding。</li>
                            </ul>

                            <h6>3.2.2 标度和高分辨率子图像的混合</h6>
                            <p>通过上述联合混合策略，SPHINX 已经在各种视觉感知和推理任务上展示了卓越的性能。然而，仍然存在一个关键挑战，即输入图像的分辨率有限。为了解决这个问题，作者进一步建议利用高分辨率子图像的混合视觉 Token，如下图 Figure 4 所示：</p>
                            <div style="text-align: center"><img src="../images\VLM\3-2.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（1）低分辨率约束（Low-resolution constraints of MLLMs）</p>
                            <p>
                                当前很多 LLM 在预训练期间会冻结 vision encoder，以保持预训练的视觉语义，因此图像分辨率还会维持在 224x224，这阻碍了其对细粒度视觉语义感知的能力，尤其是区域级的定位和描述。此外，直接对图像上采样也不是最佳选择：
                            </p>
                            <ul>
                                <li style="list-style-type: disc">为了对齐图像大小，需要对 ViT 中预训练的位置编码矢量进行相应的上采样，这会损害先前的空间能力。</li>
                                <li style="list-style-type: disc">ViT 的计算复杂度与输入图像大小呈二次方关系，因此直接上采样会导致推理时间和 GPU 显存消耗的增加。</li>
                            </ul>
                            <p style="font-weight: bold">（2）混合多尺度和高分辨子图像（Mixed scales and high-resolution sub-images）</p>
                            <p>在 SPHINX 中，作者将混合的视觉嵌入扩展到多尺度和高分辨率子图像，从而实现高效的高分辨率图像编码，其中子图像（sub-images）用于捕获细粒度的局部视觉信息，低分辨率全图（low-resolution image）用于捕获全局视觉信息，不同的图像之间在 vision encoder 上没有交互，因此它们被迫在 LLM 内交互以获得完整的视觉信息，这种策略反过来又会激励 LLM 学习视觉条件下的关系，以实现更好的跨模态学习：</p>
                            <ul>
                                <li style="list-style-type: disc">SPHINX：原始图像直接缩放到 224x224，对应 289（32 + 257）个 Token</li>
                                <li style="list-style-type: disc">SPHINX-1k：原始图像缩放到 448x448，然后切分为 4 个 224x224 子图，同时原图缩放为 224x224，相当于 5 个 224x224 的输入，对应 1445 个 Token</li>
                                <li style="list-style-type: disc">SPHINX-2k：原始图像缩放到 672x672，然后切分为 9 个 224x224 子图，同时原图缩放为 224x224，相当于 10 个 224x224 的输入，对应 2890 个 Token</li>
                            </ul>

                            <h5>3.3 Extensions to Wider Applications</h5>
                            <p style="font-weight: bold">（1）与 SAM 和 Stable Diffusion 集成（Integration with SAM and Stable Diffusion）</p>
                            <p>除了多种任务的视觉指令遵循外，作者还将 SPHINX 与其他视觉基础模型集成，以应对更有挑战的任务。</p>
                            <ul>
                                <li style="list-style-type: disc"><b>语言指示分割</b>（Language-referred segmentation）：鉴于 SPHINX 可以基于用户提供的描述输出语义类别的准确检测框，因此可以级联 SAM 进行语言指示实例或语义分割。具体来说，将 SPHINX 预测的边界框视为框提示，并输入到 SAM 中，以便对相应的实例进行分割，如下图 Figure 5 所示：</li>
                                <div style="text-align: center"><img src="../images\VLM\3-3.png" alt="" style="zoom:80%"></div>
                                <li style="list-style-type: disc"><b>图像修复和编辑</b>（Image inpainting and editing）：根据 SAM 的分割结果，参考 Inpaint Anything 来集成图像修复模型和文本到图像生成模型用于高质量的图像修复和编辑。具体来说，首先通过 SPHINX 和 SAM 检测和分割用户指示的对象，如上一部分。然后，将分割掩码输入 LaMa，以使用上下文数据删除相应的对象。在此之后，用户可以提示 Stable Diffusion 进一步生成新的视觉内容以替换原始内容。</li>
                                <div style="text-align: center"><img src="../images\VLM\3-4.png" alt="" style="zoom:80%"></div>
                            </ul>
                            <p style="font-weight: bold">（2）针对视觉识别微调（Finetuning SPHINX for Visual Recognition）</p>
                            <p>可以将 SPHINX 作为传统视觉任务的通用初始化，例如，对应 ImageNet-1K 的分类任务，可以将其转换为 “Classify the image” 的单轮对话格式，并使用 “This is a [CLASS]” 作为响应。通过对文本转换后的数据集进行监督微调，能观察到在 ImageNet-1K 上的快速训练收敛。仅使用一个 epoch，SPHINX 就可以在没有任何数据增强的情况下达到 70.8% 的分类准确率。</p>

                            <h5>3.4 Discussion</h5>
                            <p><b>（1）图文联合预训练</b>（Image-text joint pre-training）： 从实验可以看出，如果只使用多模态数据训练，会导致 LLM 对纯文本预测性能的下降（灾难性遗忘）。</p>
                            <div style="text-align: center"><img src="../images\VLM\3-5.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（2）评估 prompt 设计（Evaluation prompt design</p>
                            <div style="text-align: center"><img src="../images\VLM\3-6.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（3）多模态基准评估（Benchmarks on multi-modal large language models）</p>
                            如下图 Table 2 所示为在 10 个多模态基准上的评估结果，可以看出，SPHINX 在 10 个基准中的 6 个上面获得了 SOTA。在 POPE 指标上的结果表明，提升分辨率可以大幅缓解模型的幻觉问题。
                            <div style="text-align: center"><img src="../images\VLM\3-7.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（4）视觉问答（Visual question answering）</p>
                            <p>如下图 Table 3 所示为作者在多个 VQA 任务上的评估结果，可以看出在大部分任务上都获得了很有竞争力的结果，在面向文本的 VQA 上还弱于 Qwen-VL，主要是<b>缺乏文本相关的预训练数据</b>。</p>
                            <div style="text-align: center"><img src="../images\VLM\3-8.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（5）视觉定位（Visual grounding）</p>
                            <p>如下图 Table 4 所示为视觉定位的结果，可以看出，使用更大分辨率后在所有指标上获得 SOTA：</p>
                            <div style="text-align: center"><img src="../images\VLM\3-9.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（6）混合模型权重（Mixed model weights）</p>
                            <p>论文中并没有提供混合模型权重的结果，不过在 github 的 issue 中有提到，可以参考 https://github.com/Alpha-VLLM/LLaMA2-Accessory/issues/102</p>
                            <p style="font-weight: bold">（7）混合视觉编码器（Mixed Visual Encoder）</p>
                            <p>论文中并没有提供混合视觉编码器的结果，不过在 github 的 issue 中有提到，可以参考 https://github.com/Alpha-VLLM/LLaMA2-Accessory/issues/102</p>

                            <h5>3.5 Application</h5>
                            <p style="font-weight: bold">（1） SAM 增强实例分割（SAM-augmented instance segmentation）</p>
                            <div style="text-align: center"><img src="../images\VLM\3-10.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（2）区域级理解（Region-level understanding）</p>
                            <div style="text-align: center"><img src="../images\VLM\3-11.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（3）通过提示提升物体检测（Improved object detection through hints）</p>
                            <p>作者进一步验证了 SPHINX 在理解用户提示方面的熟练程度。如下图所示，最初让模型预测图像中所有狗会导致错误的识别，然而，在提供有关物体的额外提示后，SPHINX 很好地理解了指令，并准确预测出图像中的所有狗。</p>
                            <p style="font-weight: bold">（4）通过物体检测实现更好的视觉推理（Better visual reasoning with object detection）</p>
                            <p>如下图展示了具有物体检测功能的增强视觉推理能力。值得注意的是，SPHINX 通过最初的指令提示生成了物体检测结果，然后再次请求模型基于检测结果回答问题，获得了不错的结果。这一点证明了多任务训练中，不同任务间的协同作用可以显著提升整体表现。</p>
                            <div style="text-align: center"><img src="../images\VLM\3-12.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（5）指示姿态估计（Emergent referring pose estimation）</p>
                            <p>原始的指示物体理解和姿态估计是两个不同的任务。有趣的是，作者通过混合微调，实现了指示姿态估计的涌现能力，即直接从文本描述生成人体关键点。这观察表明，SPHINX 完全理解了不同视觉语言任务的语义，并通过卓越的推理能力将它们隐含地联系起来。</p>
                            <p style="font-weight: bold">（6）缺陷检测（Anamoly detection）</p>
                            <p>工业界中，缺陷检测是一个非常典型的应用场景。如下图所示，SPHINX 在缺陷检测方面也表现出色，虽然没有涉及相关的训练数据，但是 LMM 仍然准确地检测出相关缺陷，表明了 SPHINX 在实际应用中的潜力。</p>
                            <div style="text-align: center"><img src="../images\VLM\3-13.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（7）Multi-level dense captioning</p>
                            <p>SPHINX 经过多样化的多任务训练，可以通过自我迭代提示的方式生成多级密集描述。</p>
                            <ul>
                                <li style="list-style-type: disc">给定输入图像，使用 “Detect all objects shown in the image” 提示 SPHINX 定位所有物体的位置。</li>
                                <li style="list-style-type: disc">然后，迭代地提示每个区域，并提示 “Please provide a short description for this region : [x1, y1, x2, y2]” 来获得每个区域的简单属性。</li>
                                <li style="list-style-type: disc">为了更深入的了解检测到的区域，作者根据检测结果截取所有图像，每个截取后的图像都独立输入 SPHINX 中，并带有两个提示：“Provide a one-sentence caption for the provided image.” 和 “Generate a detailed description about the image.”，这样，可以检测图像中显示的所有物体，并用属性、简单描述和详细描述标记所有框</li>
                            </ul>
                            <div style="text-align: center"><img src="../images\VLM\3-14.png" alt="" style="zoom:80%"></div>

                            <h3 id="section4" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://arxiv.org/abs/2310.03744">4. LLaVA-v1.5-13B</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: <a href="https://llava.hliu.cc/">llava.hliu.cc</a>
                                <li style="list-style-type: circle">Time: 2023.10.05</li>
                                <li style="list-style-type: circle">Company: University of Wisconsin–Madison</li>
                                <li style="list-style-type: circle">Parameters: 13.4B</li>
                                <li style="list-style-type: circle">Language Model: Vicuna-v1.5-13B</li>
                                <li style="list-style-type: circle">Vision Model: CLIP ViT-L/14</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/haotian-liu/LLaVA
                                </li>
                                <li style="list-style-type: circle">Paper: https://arxiv.org/pdf/2310.03744.pdf</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/6B1qMvLcy_99aKM-Xr1IMw
                                </li>
                            </ul>
                            <h5>4.1 改进的LLaVA基线</h5>
                            <p>作为视觉指令微调的先驱，LLaVA 在视觉推理能力方面表现了很强的能力，在现实世界的指令遵循任务上也超过了一些最近的模型，仅在一些要求比较简短答案的学术基准（例如单个词）上效果一般。导致学术基准效果不佳的主要原因是 LLaVA 没有像其他方法那样在大规模数据上进行预训练。</p>
                            <p>如下图 Table 1 所示，作者研究了数据、模型规模以及图像分辨率等对模型的影响，并在三个评估集 GQA、MME 和 MM-Vet 上进行了效果评估。</p>
                            <div style="text-align: center"><img src="../images\VLM\4-1.png" alt="" style="zoom:50%;margin: auto" ></div>
                            <p>作者进一步在 12 个不同的基准上将最终的模型和现有的 LMM 进行比较。结果表明，LLaVA 的结构在视觉指令微调方面非常强大，有很高的数据效率，并且使用比其他所有方法少的多的计算和训练数据实现了 SOTA。</p>
                            <div style="text-align: center"><img src="../images\VLM\4-2.png" alt="" style="zoom:80%"></div>
                            <h5>4.2 格式化响应提示（Response formatting prompts</h5>
                            <p>作者发现，InstructionBLIP 中采用的方法无法在短序列和长序列 VQA 之间取得平衡，主要原因有：</p>
                            <ul>
                                <li style="list-style-type: disc"><b>对响应格式的提示不明确</b>，例如， “Q: {Question} A: {Answer}”。此类提示并不能清楚地表明所需的输出格式，并且，即使对于自然的视觉对话，也可能使 LLM 输出过度精简类型的答案。</li>
                                <li style="list-style-type: disc"><b>没有对 LLM 进行微调</b>。其中的第一个问题也因 InstructBLIP 在指令微调阶段仅微调 Qformer 而进一步恶化。它要求 Qformer 的视觉输出 Token 来控制 LLM 输出为长格式或者短格式，如 prefix-tunning，但是与 LLaMA 相比，<b>Qformer 可能缺乏正确执行此操作的能力。</b></li>
                            </ul>
                            <p>相关定性示例可以参考如下图 Table 6：</p>
                            <div style="text-align: center"><img src="../images\VLM\4-5.png" alt=""  style="zoom:90%;margin: auto"></div>
                            <p>为了解决这一问题，作者建议使用一个明确指示输出格式的<b>单一响应格式提示</b>，当期望简短回答问题时，
                                可以将如下提示扩展到问题最后：“Answer the question using a single word or phrase.”。
                                经验表明，当 LLM 使用此类提示进行微调时，LLaVA 能够根据用户指令正确调整输出格式，并且不需要使用 ChatGPT 等对 VQA 数据进行额外处理，
                                这也可以扩展到各种数据源。如下图 Table 1 所示，仅仅在<b>训练中添加 VQAv2</b>，
                                LLaVA 在 MME 上的性能显著提高，比 InstructBLIP <b>高 111 分</b>（502.8 -> 1323.8）。</p>
                            <h5>4.3 MLP 视觉-语言连接器（MLP vision-language connector）</h5>
                            <p>在之前的对比学习中，多个工作都表明将线性投影（Linear projection）换成 MLP 可以提升自监督学习性能，受此启发，作者将原来的线性投影换成两层 MLP 来提高视觉-语言连接器的表达能力，用于进一步提升 LLaVA 的多模态能力。</p>
                            <h5>4.4 面向学术任务的数据集（Academic task oriented data）</h5>
                            <p>作者进一步增加了针对 VQA、OCR 和区域感知的面向学术任务的数据集，以各种方式来增强模型的能力。如下图 Table 1 所示，作者添加了 InstructBLIP 中使用的另外 4 个数据集：开放知识 VQA（OKVQA、A-OKVQA）和 OCR（OCRVQA、TextCaps）。A-OKVQA 被转换为多项选择题，并使用特定的响应格式提示：“Answer with the option’s letter from the given choices directly. ”。虽然只使用了 InstructBLIP 使用数据集的子集，但是 LLaVA 在所有的 3 个任务上都超过了 InstructBLIP（红色框），这也表明了 LLaVA 的有效性。此外作者还进一步添加了区域级 VQA 数据集（Visual Genome 和 RefCOCO），其可以提升模型定位细粒度视觉细节的能力（紫色框）。结果如Table 1 所示</p>
                            <h5>4.5 增加规模（Additional scaling）</h5>
                            <p>作者进一步扩大了图像的分辨率，从 <b>224 扩展到 336</b>，使 LLM 能更清楚地“看到”图像的细节，
                                并<b>添加 GQA 数据集</b>，做为额外的视觉知识来源。此外还<b>合并了 ShareGPT 数据</b>，
                                并将 <b>LLM 扩展到 13B</b>。如上图 Table 1 的 [6, 7, 8，9] 可以看出，这些都对模型效果有进一步提升。
                                当 LLM 扩展到 13B 时，MM-Vet 上性能大幅提升（30.5 -> 36.3），这样表明基础 LLM 的视觉-对话能力的重要性。作者将最终的模型称为 LLaVA-1.5。</p>
                            <h5>4.6 Discussion</h5>
                            <p style="font-weight: bold">（1）对比 SOTA（Comparison with SoTA）</p>
                            <p>作者在多个学术 VQA 基准和最近专门针对指令遵循 LLM 提出的基准（总共 12 个基准）上对 LLaVA-1.5 进行了评估。结果表明，尽管只使用了更小的预训练和指令微调数据，却在 12 个基准上的 11 个中实现 SOTA。另一方面，LLaVA-1.5 结构简单，数据效率高，训练代价很小。作者论文中的结果没有与最近刚发布的几个工作比较，基于有限能对齐的指标可以看出，CogVLM 的性能看着更强大一些，当然 <b>CogVLM 的训练成本也很高</b>。</p>
                            <p style="font-weight: bold">（2） zero-shot 格式化指令泛化（Zero-shot format instruction generalization）</p>
                            <p>尽管 LLaVA-1.5 只使用有限数量的格式化指令进行训练，但它也可以推广到其他指令。首先，当提供的内容不足以回答问题时，VizWiz 要求模型输出 “Unanswerable”，作者的响应格式提示（如 Table 8）能更有效的指导模型符合这个要求，无法回答的问题中，将这个比例从 11.1% 提升到 67.8%。</p>
                            <div style="text-align: center"><img src="../images\VLM\4-6.png" alt="" style="zoom:60%;margin: auto"></div>
                            <p>作者也展示了 LLaVA-1.5 关于指示 LLaVA-1.5 来验证欺骗性问题的定性示例，如 Table 3 所示：</p>
                            <div style="text-align: center"><img src="../images\VLM\4-3.png" alt="" style="zoom:30%;margin: auto"></div>
                            <p>作者还进一提供了以受约束的 Json 格式进行响应的能力，可以看出，确实按照格式输出，<b>场景文本识别</b>相比 LLaVA-1 也进一步提升，但是<b>相比 GPT-4V 还有较大差距</b>。</p>
                            <div style="text-align: center"><img src="../images\VLM\4-4.png" alt="" style="zoom:80%;margin: auto"></div>
                            <p style="font-weight: bold">（3）zero-shot 多语言能力（Zero-shot multilingual capability）</p>
                            <p>虽然 LLaVA-1.5 根本没有针对多语言多模态指令进行微调，但是作者发现其能够遵循多模态指令，部分原因是 ShareGPT 中包含多语言指令。作者在 MMBenchCN 上定量评估了该模型对中文的泛化能力，其中 MMBench 的问题被转换为中文。值得注意的是，尽管 Qwen 针对中文多模态指令进行了微调，而 LLaVA-1.5 没有，但是 <b>LLaVA-1.5 的表现比 Qwen-VL-Chat 高出 7.3%</b>（63.6% -> 56.7%）。</p>
                            <p style="font-weight: bold">（4）计算成本（Computational cost）</p>
                            <p>对于 LLaVA-1.5，在指令微调阶段，作者使用相同的 LCS-558K 预训练数据集，并保持训练迭代 step 和 batch size 与 LLaVA-1 大致相同。由于图像输入分辨率提高到 336px，LLaVA-1.5 的训练时间增加到 LLaVA 的 2 倍：使用 8xA100 GPU，大约 6 小时的预训练和 20 小时的视觉指令微调。</p>
                            <p style="font-weight: bold">（5）限制（Limitations）</p>
                            <p>尽管 LLaVA-1.5 展示了很不错的结果，但也必须承认有一定的局限性：</p>
                            <ul>
                                <li style="list-style-type: disc">首先，LLaVA-1.5 <b>使用完整的图像块（对应 Token 个数）</b>，这可能延长每次训练迭代的时间。
                                    虽然 visual resampler（比如 Qformer 和 Qwen-VL 中）可以减少 LLM 输入的视觉块数量，
                                    但是它们目前没办法像 LLaVA 这样在相当数量的数据下实现高效收敛，这可能是 visual resampler 中的可训练参数更多。
                                    开发高效的 visual resampler 可以为未来扩展多模态指令跟随 LMM 提供更多空间。</li>
                                <li style="list-style-type: disc">其次，LLaVA-1.5 还<b>不能处理多个图像</b>，这主要是两个方面的原因：缺乏相关多模态指令数据和上下文长度的限制。</li>
                                <li style="list-style-type: disc">此外，虽然 LLaVA-1.5 在遵循复杂指令方面表现出不错的能力，但其<b>在某些领域解决问题的能力还有明显不足</b>，这可以通过使用更强大的语言模型，高质量、有针对性的视觉指令微调数据来改进。</li>
                                <li style="list-style-type: disc">最后，LLaVA-1.5 显著降低了产生幻觉的倾向，但它<b>仍会产生幻觉并偶尔传播错误信息</b>，因此在关键应用中需要保持谨慎（比如医疗）</li>
                            </ul>

                            <h3 id="section5" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2">5. mPLUG-Owl2</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: <a href="https://modelscope.cn/studios/damo/mPLUG-Owl2/summary">modelscope.cn/studios/damo/mPLUG-Owl2/summary</a>
                                <li style="list-style-type: circle">Time: 2023.11.10</li>
                                <li style="list-style-type: circle">Company: DAMO Academy</li>
                                <li style="list-style-type: circle">Parameters: 8.2B</li>
                                <li style="list-style-type: circle">Language Model: LLaMA2 7B</li>
                                <li style="list-style-type: circle">Vision Model: CLIP ViT-L/14</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/X-PLUG/mPLUG-Owl/tree/main/mPLUG-Owl2
                                </li>
                                <li style="list-style-type: circle">Paper: https://arxiv.org/pdf/2311.04257.pdf</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/DNLp6nyQH_zJXQpEB2eHKg
                                </li>
                            </ul>
                            <h5>5.1 简介</h5>
                            <p>多模态大型语言模型（MLLMs，Multi-modal Large Language Models）在各种开放式任务中表现出了令人影响深刻的指令能力。然而，以前的方法主要侧重于增强多模态能力，在这项工作中，作者介绍了一种通用的 MLLM：mPLUG-Owl2，其有效地利用模态协作来提高文本和多模态任务的性能。mPLUG-Owl2 采用模块化网络设计，language encoder 充当管理不同模态的通用接口。具体而言，mPLUG-Owl2 结合了共享的功能模块，以促进模态协作，并引入了一个模态自适应模块（modality-adaptive module），以保留模态特定的特征。大量实验证明，mPLUG-Owl2 能够泛化文本任务和多模态任务，并通过单个通用模型实现 SOTA 性能，在 8 个经典视觉语言基准上实现了 SOTA。此外，它在最近的 5 个 few-shot 多模态基准测试中的表现要么是第一，要么是第二，mPLUG-Owl2 还在多个纯文本基准测试中实现了 SOTA。</p>
                            <p>此外，作者还对提出的 modality-adaptive module 提供了深入的分析，以证明和验证模态协作的影响，特别是在增强文本任务方面，包括理解、知识和推理。最后，综合消融实验验证了所提出的 MLLM 训练范式的有效性，需要说明的是，mPLUG-Owl2 是第一个在纯文本和多模态场景中展示模态协作现象的 MLLM 模型，这也有助于启发未来多模态基础模型的发展。</p>
                            <p>先前关于多模态学习的研究表明，不同的模态可以有效协作，从而同时提高文本和多模态任务的性能。然而 MLLM 是一个单一的模型，支持不同的模式和任务，而无需对特定任务进行微调。最近的工作中常使用跨模态对齐模块（比如 Q-Former、线性层等）将视觉编码器的视觉特征映射到冻结的 LLM 中，以通过利用保留的语言能力来执行多模态任务，然而，这种模式限制模态协作的潜力。因此，也有些研究工作选择在多模态训练中微调 LLM，这显著改善了多模态任务能力，但它也有削弱文本任务性能的风险。如下图 Figure 1 所示，MLLM 中模态协作的挑战在于应用单个模块来平衡<b>模态协作</b>和<b>模态干扰</b>（模态之间可能在大量指令数据集间相互干扰）的收益。</p>
                            <div style="text-align: center"><img src="../images\VLM\5-1.png" alt="" style="zoom: 70%"></div>
                            <h5>mPLUG-Owl 模型架构</h5>
                            <p>如下图 Figure 2 所示为mPLUG-Owl2 的上一个版本 mPLUG-Owl 的模型结构，可以看出其由三个模块组成：</p>
                            <ul>
                                <li style="list-style-type: disc"><b>Vision Encoder</b>：作者采用 CLIP ViT-L/14 作为视觉主干。</li>
                                <li style="list-style-type: disc"><b>Visual Abstractor</b>：作者采用了类似 Flamingo 的 <b>Perceiver Resampler</b>结构，论文中没有介绍，在代码库的 ISSUE 中有提到 https://github.com/X-PLUG/mPLUG-Owl/issues/10，查看源码也可以看出来，在此之后有一个 Linear 层。需要注意的是，不是直接用视觉特征（Patch Feature）作为 Cross Attention 的 Key 和 Value，而是会先将 Patch Feature 与 Query 拼接（Concatenate），然后做作为 Key 和 Value。</li>
                                <li style="list-style-type: disc"><b>Large Language Model</b>：直接使用开源的 LLaMA-7B 作为语言模型主干，第二阶段会使用 LoRA 微调。</li>
                            </ul>
                           <div style="text-align: center"><img src="../images\VLM\5-2.png" alt="" style="zoom:80%"></div>
                            <h5>5.3 mPLUG-Owl2 模型架构</h5>
                            <div style="text-align: center"><img src="../images\VLM\5-3.png" alt="" style="zoom:80%"></div>
                            <p>如上图所示为 mPLUG-Owl2 模型的结构，可以看出，主要也是由三部分组成：</p>
                            <ul>
                                <li style="list-style-type: disc"><b>Vision Encoder</b>：作者采用 CLIP ViT-L/14 作为视觉主干。输入一个 H x W 分辨率的图像，生成 H/14 x W/14 的 Token 序列，对应视觉特征 I=[I1, I2,..., IP]，维度为 P x d，P 表示 Token 数目，d 表示特征维度。其也会作为 Visual Abstractor 的输入。</li>
                                <li style="list-style-type: disc"><b>Visual Abstractor</b>：和上一版本 mPLUG-Owl 采用了一样的结构，也包含一组可学习的 Query（本文数目为 64）， 作用也一样，降低 Token 数目，同时实现和文本特征的对齐。需要注意的是，不是直接用视觉特征 I 作为 Cross Attention 的 Key 和 Value，而是会先将 I 与 Query 拼接（Concatenate），然后作为 Key 和 Value。</li>
                                <div style="text-align: center"><img src="../images\VLM\5-4.png" alt="" style="zoom: 80%"></div>
                                <li style="list-style-type: disc"><b>Large Language Model</b>：使用 LLaMA-2-7B 作为 LLM encoder，作者对 LLM encoder 中的 Self-Attention 进行了修改，叫做<b>Modality Adaptive Module</b>，后续会详细介绍。</li>
                            </ul>
                            <h5>5.4 模态自适应模块（Modality-Adaptive Module）</h5>
                            <p>之前的 MLLM 中，通常是通过将图像特征投影到语言语义空间来使视觉特征与语言特征保持一致，然而，这种策略可能导致粒度不匹配，与文本特征中的离散语义信息相比，图像特征中通常包含更丰富的语义信息。这些方法忽略了视觉与文本信息的独特特征，从而限制了模型的性能。为此，作者提出了 Modality Adaptive Module，其可以<b>将视觉特征和语言特征投射到共同的语义空间中，同时保留每个模态的独特属性来解耦视觉-语言表示。</b></p>
                            <p>与传统的 Transformer decoder 有几点不同：</p>
                            <ul>
                                <li style="list-style-type: disc">给定上一层的输出 $H_{l-1}$，分别对视觉模态和语言模态进行 LayerNorm，其中 M 表示模态指示，0 表示视觉模态，1 表示语言模态。$LN_V$ 和 $LN_T$ 表示 layer normalization。</li>
                                <li style="list-style-type: disc">Q、K、V 分别乘上对应的权重矩阵 W，需要注意的是，K 和 V 针对视觉模态和语言模态有独立的权重矩阵 W，而 Q 的权重矩阵是共享的（此处与 CogVLM 不同，CogVLM 的 Q、K、V 都有独立的视觉、语言权重矩阵），然后就是正常的 Attention。</li>
                                <div style="text-align: center"><img src="../images\VLM\5-6.png" alt="" style="zoom:80%;"></div>
                                <li style="list-style-type: disc">最后输出之前还会和第 1 步类似，分别执行 layer normalizaton</li>
                                <li style="list-style-type: disc">之后就是接正常的 FFN，不过不属于本模块</li>
                            </ul>
                            <h5>5.6 训练流程（Training Paradigm）</h5>
                            <p>mPLUG-Owl2 的训练也包含两阶段，类似于其他的 MLLM 模型，第一阶段为预训练，第二阶段为指令微调。预训练阶段的主要目的是实现视觉编码器和语言模型的对齐，指令微调阶段是为了改进其指令遵循能力。然而，简单冻结 Visual Encoder 只训练视觉 projector 来实现视觉与语言模态对齐会限制它们理解复杂视觉信息的能力，如场景文本和视觉知识。为了解决这一问题，在整个预训练和指令微调阶段都会打开 Visual Encoder，这有助于模型更有效地捕获低级和高级视觉语义信息。具体来说：</p>
                            <ul>
                                <li style="list-style-type: disc">预训练：Visual Encoder、Visual Abstractor 都是可训练的，Language-Decoder 中只有 Modality Adaptive Module 内新增的权重（视觉相关权重）可训练。</li>
                                <li style="list-style-type: disc">指令微调：对整个模型进行 finetune，以同时结合文本和多模态指令。</li>
                            </ul>
                            <h5>5.7 Discussion</h5>
                            <p style="font-weight: bold">（1）图像描述和视觉问答（Image Caption and Visual Question Answering）</p>
                            <p>作者使用一系列用于评估视觉-语言模型的基准来评估 mPLUG-Owl2。评估了 8 个流行的基准，结果表明，mPLUG-Owl2 在描述和问答任务方面都超过以前的 MLLM。具体而言，mPLUG-Owl2 在 Flickr30K 数据集上实现了 SOTA 性能，即使与具有更强大骨干的模型（例如，Qwen-VL Chat 和 InstructBLIP）相比也是如此。此外，mPLUG-Owl2 在视觉问答方面表现出明显的优势，尤其是在无 OCR 的场景中， mPLUG-Owl2 以 zero-shot 的方式在 TextVQA 数据集上实现了 54.3 的准确率，证明了训练策略的优势。同样值得注意的是，mPLUGOwl2 在 ScienceQA（Image Set）和 VizWizQA 数据集上也显示出强大的 zero-shot 性能。</p>
                            <p style="font-weight: bold">（2）面向 MLLM 的多模态基准（MLLM-oriented Multi-modal Benchmarks）</p>
                            <p>鉴于 MLLM 强大的 zero-shot 能力，传统的评估指标往往无法提供详细的能力评估。这进一步导致它们无法准确匹配给定的答案，导致严重的鲁棒性问题。为了应对这些挑战，研究界引入了一系列基准测试，包括 MME、MMBench、MM-Vet、SEED-Bench 和 Q-Bench。作者以 zero-shot 方式将提出的模型应用于这 5 个基准。为了进行公平的比较，作者选择了语言模型大小相似的模型，特别是 LLaMA 系列的模型，并详细说明了它们在 Vision Encoder 中的差异（mPLUG-Owl2 中扩展了视觉相关权重，参数量应该不只是 7B，应该多 10% 左右？）。</p>
                            <p>评估结果：mPLUG-Owl2 在 MMBench、MM-Vet 和 Q-Bench 上实现了更高的 zero-shot 性能。相反，MME 的性能较低，这可能是 MME 中的测试样本数量有限，导致性能的敏感波动。特别是，在 Q-Bench 上表现出显著改进，Q-Bench 是检查 MLLM 低级视觉感知的基准。当应用较小的视觉骨架（即 ViT-L）时，就会发生这种改善，从而增强低级视觉感知。这证明了本文用于训练视觉骨干的策略的有效性。</p>
                            <p style="font-weight: bold"><b>（3）自然语言理解和生成（Natural Language Understanding and Generation）</b></p>
                            <p>通过利用 LLM 的强大能力，当前的 MLLM 通常在各种多模态下游任务中表现出色。这些模型的内在能力往往在决定 MLLM 的性能方面发挥着重要作用，然而，这在以前的 MLLM 研究中经常被忽视。因此，作者还评估了 mPLUG-Owl2 在自然语言理解和生成方面的表现。作者在 MMLU、BBH、AGIEval 和 ARC 上进行了评估。</p>
                            <p>从结果可以看出，mPLUG Owl2 在考试和推理方面表现出色，在 MMLU 和 BBH 比最好的 Vicuna-v1.5 分别提高了 2.3（51.1->53.4） 和 3.8（41.2->45.0）。这表明 mPLUG-Owl2 不仅在多模态任务上表现良好，而且与其他指令调优的 LLM 相比，它也获得了更好的性能，这为开发强大的 MLLM 提供了一条很有前途的途径。</p>
                            <p style="font-weight: bold"><b>（4）Zero-shot 视频问答（Zero-Shot Video Question Answering）</b></p>
                            <p>鉴于视频可以看做一系列图像，作者使用几个常用的视频问答数据集进行了全面的定量评估，包括 MSRVTT-QA、MSVD-QA 和 TGIF-QA。这些数据集有助于对模型理解视频内容的能力进行 zero-shot 评估，作者采用了两种类型的评估：</p>
                            <ul>
                                <li style="list-style-type: disc">精确匹配，这在以前的视频问答评估中经常使用。</li>
                                <li style="list-style-type: disc">GPT 辅助评估，通过测量模型生成的预测的准确性并提供 1-5 分的相对分数来评估模型的能力。</li>
                            </ul>
                            <p>mPLUG-Owl2 在 zero-shot 设置下在所有三个视频数据集上都取得了优异的结果。此外，在相关性方面，mPLUG-Owl2 比其他视频 MLLM 生成了更准确的答案，从而证明了其优越性和出色的泛化能力。</p>

                            <h3 id="section6" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="#">6. GPT-4V</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Time: 2023.9.25</li>
                                <li style="list-style-type: circle">Company: Open AI</li>
                                <li style="list-style-type: circle">Paper: https://arxiv.org/pdf/2309.17421.pdf</li>
                                <li style="list-style-type: circle">Reference:
                                    https://zhuanlan.zhihu.com/p/659522826
                                </li>
                            </ul>
                            <h5>6.1 GPT-4V的用法</h5>
                            <p>5种使用方式：输入图像（images）、子图像（sub-images）、文本（texts）、场景文本（scene texts）和视觉指针（visual pointers）。</p>
                            <p>3种支持的能力：指令遵循（instruction following）、思维链（chain-of-thoughts）、上下文少样本学习（in-context few-shot learning）。</p>
                            <h5>6.2 GPT-4V在10大任务中的表现</h5>
                            <p>开放世界视觉理解（open-world visual understanding）、视觉描述（visual description）、多模态知识（multimodal knowledge）、常识（commonsense）、场景文本理解（scene text understandin）、文档推理（document reasoning）、写代码（coding）、时间推理（temporal reasonin）、抽象推理（abstract reasoning）、情感理解（emotion understanding）</p>
                            <h5>6.3 类GPT-4V多模态大模型的提示词技巧</h5>
                            <p>提出了一种新的多模态提示词技巧“视觉参考提示”（visual referring prompting），可以通过直接编辑输入图像来指示感兴趣的任务，并结合其他提示词技巧使用。</p>
                            <h5>6.4 多模态大模型的研究&落地潜力</h5>
                            <p>预测了多模态学习研究人员应该关注的2类领域，包括落地（潜在应用场景）和研究方向， 比如故障检测。</p>
                            <h5>具体内容可以参看技术报告</h5>

                            <h3 id="section7" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="#">7. CogVLM</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: 36.103.203.44:7861</li>
                                <li style="list-style-type: circle">Time: 2023.10.27</li>
                                <li style="list-style-type: circle">Company: 智谱AI&清华KEG</li>
                                <li style="list-style-type: circle">Parameters: 17B</li>
                                <li style="list-style-type: circle">Language Model: Vicuna-7Bv1.5</li>
                                <li style="list-style-type: circle">Vision Model: EVA2-CLIP-E</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/THUDM/CogVLM
                                </li>
                                <li style="list-style-type: circle">Paper: https://github.com/THUDM/CogVLM/blob/main/assets/cogvlm-paper.pdf</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/1zbMNKDYa91HTGxVth3w5A
                                </li>
                                <li style="list-style-type: circle">具体细节和问题可以参考：https://zhuanlan.zhihu.com/p/662011803</li>

                            </ul>
                            <h5>7.1 简介</h5>
                            <p>VLMs可以用于许多视觉和跨模态任务，如图像字幕、视觉问答、视觉定位和分割等。这些任务可以被看作是下一个标记预测问题。本文介绍了如何从预训练的语言模型中训练出视觉语言模型（VLM），VLM可以在视觉和语言任务中取得更好的表现。然而，从头开始训练一个VLM已经很困难了，更难的是训练出与已经训练好的纯语言模型相同的NLP性能。因此，本文探讨了如何从现成的预训练语言模型中训练出VLM。</p>
                            <p>BLIP-2是一种流行的浅层对齐方法，通过可训练的Q-Former或线性层将图像特征映射到语言模型的输入嵌入空间中。这种方法收敛速度快，但性能不如联合训练视觉和语言模块的方法。对于聊天式的VLM，浅层对齐方法的弱视觉理解能力表现为产生幻觉。因此，是否可能在保留大型语言模型的NLP能力的同时，为其添加一流的视觉理解能力呢？</p>
                            <p>CogVLM认为，浅层对齐方法表现不佳的根本原因在于<b>视觉和语言信息之间缺乏深度融合</b>。这启发了作者从p-tuning和LoRA的比较中得到灵感，LoRA表现更好，因为它通过低秩矩阵适应每层的模型权重。类似的现象可能也存在于VLM中。浅层对齐方法中，图像特征就像p-tuning中的前缀嵌入。导致p-tuning和浅层对齐表现下降的原因包括：</p>
                            <ul>
                                <li style="list-style-type: disc">语言模型中的冻结权重是为文本标记进行训练的。在输入文本空间中，视觉特征并没有完美的对应。因此，经过多层变换后，视觉特征可能不再匹配深层权重的输入分布。</li>
                                <li style="list-style-type: disc">在预训练时，浅层对齐方法只能将图像描述任务的先验信息，如写作风格和描述长度编码到视觉特征中。它削弱了视觉特征和内容之间的一致性。</li>
                            </ul>
                            <p>PaLI和Qwen-VL采用了图像-文本联合训练的方法，但这可能会影响自然语言处理能力。PaLM-E研究表明，在VLM预训练期间训练语言模型会导致灾难性遗忘，对于8B语言模型，NLG性能下降了87.3%。</p>
                            <ul>
                                <li style="list-style-type: disc">CogVLM是一种在语言模型中添加可训练的视觉专家的方法，可以在不增加FLOPs的情况下增加参数数量。如果输入序列不包含图像，则行为与原始语言模型相同。</li>
                                <li style="list-style-type: disc">CogVLM-17B在14个跨模态基准测试中表现出最先进或第二好的性能，包括图像字幕、VQA、视觉定位和多项选择等数据集。CogVLM-28B-zh支持英文和中文商业用途。</li>
                                <li style="list-style-type: disc">CogVLM是一个开源的视觉语言模型，这对于视觉理解的研究和工业应用有很大帮助。此前的大多数著名VLM都是闭源的，CogVLM的开源将有助于推动视觉理解的发展。</li>
                            </ul>
                            <h5>7.2 模型架构</h5>
                            <p>CogVLM模型包括四个基本组件：视觉转换器（ViT）编码器、MLP适配器、预训练的大型语言模型（GPT）和视觉专家模块。CogVLM架构的概述如Figure 3所示。组件的设计和实现细节如下：</p>
                            <div style="text-align: center"><img src="../images\VLM\7-1.png" alt="" style="zoom:80%"></div>
                            <ul>
                                <li style="list-style-type: disc"><b>ViT编码器。</b>CogVLM-17B使用预训练的EVA2-CLIP-E模型，其中ViT编码器的最后一层被移除，因为它专门用于聚合[CLS]特征进行对比学习。</li>
                                <li style="list-style-type: disc"><b>MLP适配器</b>是一个两层的MLP，用于将ViT的输出映射到与单词嵌入的文本特征相同的空间。所有图像特征在语言模型中共享相同的位置ID。</li>
                                <li style="list-style-type: disc"><b>预训练大语言模型</b>采用Vicuna-7Bv1.5进行进一步训练，其中包括对图像特征的注意力操作。</li>
                                <li style="list-style-type: disc"><b>视觉专家模块</b>通过在每个层中添加视觉专家模块，实现了对图像特征的转换，使其与语言模型的注意力头对齐，从而实现深度融合。视觉专家模块由QKV矩阵和MLP组成，与预训练语言模型的形状相同，并从中初始化。</li>
                            </ul>
                            <p>输入的隐藏状态X是一个四维张量，包含了图像和文本序列的隐藏状态。其中，B是批次大小，L I 和L T 分别是图像和文本序列的长度，H是注意力头的数量，D是隐藏大小。在带有视觉专家的注意力中，X首先被分割为图像隐藏状态$X_I$ 和文本隐藏状态$X_T$ ，然后计算注意力。</p>
                            <div style="text-align: center"><img src="../images\VLM\7-2.png" alt="" style="zoom:80%"></div>
                            <p>视觉专家在FFN层中的表现类似。</p>
                            <div style="text-align: center"><img src="../images\VLM\7-3.png" alt="" style="zoom:80%"></div>
                            其中FFN I和FFN T是两者的神经网络。
                            <h5>7.4 预训练</h5>
                            <p><b>数据。</b>预训练的图像-文本对都是公开可用的，包括LAION-2B和COYO-700M。经过筛选，剩下约15亿张图像用于预训练。</p>
                            <p>研究人员构建了一个包含40M张图像的视觉 grounding 数据集，每个名词都与边界框相关联，用于指示其在图像中的位置。该数据集的构建过程基本上遵循了 Peng 等人的方法，使用 spaCy 提取名词并使用 GLIPv2 预测边界框。这些图像-文本对是从 LAION-115M 中采样的，保留了至少包含两个边界框的图像的子集。</p>
                            <p><b>训练。</b>第一阶段训练使用1.5B的图像-文本对进行，共进行了120,000次迭代。第二阶段训练使用VQA形式的文本-图像对和视觉定位数据集，共进行了60,000次迭代。最终30,000次迭代中，输入分辨率从224×224变为490×490。模型总参数量为6.5B，预训练消耗了约4,096个A100×days。</p>
                            <h5>7.5 对齐</h5>
                            <p>CogVLM被进一步微调，以适应各种任务，使其能够与任何主题的自由形式指令对齐。本文将微调后的模型命名为CogVLM-Chat。如图2和附录中的示例所示，CogVLM-Chat可以成功地与各种指令对齐，从而实现与人类的灵活互动。</p>
                            <p><b>数据。</b>有监督微调（SFT）所需的高质量数据来源，包括LLaVA-Instruct、LRV-Instruction、LLaVAR和内部数据集，共约50万个VQA对。其中，LLaVA-Instruct由仅涉及语言的GPT-4生成，难免存在错误，因此需要通过手动检查和注释进行纠正。</p>
                            <p><b>SFT</b>是指监督微调，使用批量大小为640、学习率为$10^{-5}$和50个热身迭代进行8000次迭代训练。</p>
                            <p>为了防止过拟合，本文在预训练语言模型的更新中使用了较小的学习率（其他参数的10%）。在SFT期间，除了ViT编码器外，所有参数都是可训练的。</p>
                            <div style="text-align: center"><img src="../images\VLM\7-4.png" alt="" style="zoom: 110%"></div>
                            <h5>7.6 Discussion</h5>
                            <p style="font-weight: bold">（1）图像标题生成</p>
                            <p>评估了预训练基础模型在四个基准测试中的图像字幕能力。在Nocaps和Flickr数据集上进行零-shot评估，评估模型描述长尾视觉概念的准确性。此外，论文还展示了在COCO和TextCaps数据集上微调的结果。</p>
                            <p>模型在各项指标上达到了SOTA或兼容的表现。在NoCaps基准测试中，本文基础模型在四个分组中表现优于之前最好的方法GIT2，其中在out-domain数据集中最高提升了5.7个点，而只使用了10%的预训练数据（1.5B vs 12.9B）。在Flickr基准测试中，模型取得了94.9的SOTA分数，超过了同时发布的Qwen-VL模型9.1个点。这些结果展示了预训练模型在该任务上的显著能力和稳健性。作者还在COCO和TextCaps上进行了评估，其中TextCaps专门设计用于将给定图像的文本信息整合到字幕中。令人鼓舞的是，尽管没有使用专门的OCR数据进行训练，模型展现出了显著的文本阅读能力，并且在与PaLI-X-55B的竞争中取得了有竞争力的表现，比同样规模的之前最好模型PaLI-17B提高了9.1个点。</p>
                            <p style="font-weight: bold">（2）视觉问答</p>
                            <p>视觉问答是验证模型多模态能力的任务，需要掌握视觉语言理解和常识推理等技能，在7个VQA基准测试上评估模型：VQAv2、OKVQA、GQA、VizWiz-QA、OCRVQA、TextVQA、ScienceQA，涵盖了广泛的视觉场景。在训练集上训练基础模型，并在所有基准测试的公开验证/测试集上进行评估，两个过程都采用开放词汇生成设置，不使用OCR管道输入。</p>
                            <p>模型在6个基准测试中取得了最先进的性能，超过了类似规模的模型PALI-17B和Qwen-VL。模型甚至在多个基准测试中超过了规模更大的模型，如VizWiz-QA上的PaLI-X-55B（测试标准+5.1，测试开发+3.8），VQAv2上的PALM-E-84B（测试开发+4.2）和OKVQA（+1.4），VQAv2上的Flamingo-80B（测试开发+2.7，测试标准+2.6），VizWiz-QA（测试开发+10.7，测试标准+10.4）和TextVQA（+15.6）。本模型还在ScienceQA的多模态分割（即IMG）上实现了92.71的最优分数，达到了新的最先进水平。这些结果表明模型可以作为一个强大的多模态骨干，能够解决各种视觉问答任务。</p>
                            <p style="font-weight: bold">（3）视觉定位</p>
                            <p>CogVLMGrounding是一个通用的视觉 grounding 模型，使用了高质量的数据集进行预训练和训练。不同的数据集可以适应不同的任务，但有些转换可能会导致歧义。CogVLM在标准视觉 grounding 基准测试中取得了最先进的性能，并在各个任务上都有显著优势。此外，本文还评估了模型在每个单独的训练集上的性能，结果表明模型在五个子集上取得了最佳表现。这表明本训练范式具有出色的视觉 grounding 能力。</p>
                            <p style="font-weight: bold">（4）现实世界中用户行为的指令遵循</p>
                            <p>CogVLM-Chat模型在TouchStone基准测试中表现优异，GPT-4相似度得分高于其他公开可用的VLM。</p>
                            <p style="font-weight: bold">（5）消融实验</p>
                            <p><b>模型架构和微调参数。</b>本文研究了调整MLP Adapter层或调整所有LLM参数和Adapter而不添加VE的有效性，以及修改VE架构以在每4个LLM层或仅在所有层中添加FFN-equipped VE。结果表明，仅调整Adapter层可能会导致浅层对齐，性能显著下降，减少VE层数或每个LLM层的VE参数会导致明显的降级。</p>
                            <p><b>初始化方法。</b>研究表明，从LLM中初始化VE权重的方法对性能有积极影响，尽管效果略有下降。</p>
                            <p><b>视觉注意力掩码。</b>使用因果性掩码在视觉令牌上会比使用完全掩码产生更好的结果。假设这种现象的可能解释是因为因果性掩码更符合LLM的内在结构。</p>
                            <p><b>图像自监督学习损失。</b>研究发现，在图像特征上进行自监督学习损失并不能提升下游任务的表现，尽管在早期实验中确实观察到了小模型的改进。</p>
                            <p><b>指数移动平均。</b>在预训练中使用EMA（指数移动平均），这通常可以在各种任务中带来改进。</p>

                            <h3 id="section8" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://huggingface.co/BleachNick/MMICL-Instructblip-T5-xxl">8. MMICL</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: http://www.testmmicl.work/</li>
                                <li style="list-style-type: circle">Time: 2023.8.18</li>
                                <li style="list-style-type: circle">Company: 北京交通大学、北京大学、UCLA、足智多模公司等机构联合推出</li>
                                <li style="list-style-type: circle">Parameters: 12.3B</li>
                                <li style="list-style-type: circle">Language Model: FLANT5-XXL</li>
                                <li style="list-style-type: circle">Vision Model: EVA-G</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/HaozheZhao/MIC
                                </li>
                                <li style="list-style-type: circle">Paper: https://arxiv.org/abs/2309.07915</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/L9JfoDxOLcpen2PxBZ4YTw
                                </li>
                            </ul>
                            <h5>8.1 简介</h5>
                            <p>大多数vlm利用视觉提示生成器（VPG）从视觉主干编码的图像特征中提取视觉嵌入，并使用视觉嵌入来帮助llm理解视觉输入。Figure 2.a所示的模型架构属于专注于单个图像提示的VLM，如Blip-2，它总是将图像放在整个输入的顶部，不能处理多个图像的输入，具有few-shot能力的VLM，如Flamingo，将图像编码为具有固定数量的视觉标记的图像嵌入，并在LLM中使用交叉注意来混合视觉和文本内容。与之前的工作不同，图2.c中所示的MMICL平等地对待图像和文本表示，并通过图像声明image declaration建立了图像和文本之间的引用。它使用户能够灵活地以任何所需的顺序输入多个图像和文本，而不受上下文中图像的数量或放置的限制。每个给定的图像都由一个视觉编码器，如ViT进行编码，以获得图像表示。然后，作者使用Q-former作为VPG来提取视觉嵌入，利用一个全连接层作为投影层，将每个视觉嵌入转换为与LLM的文本嵌入相同的维度。最后，将多个图像的视觉嵌入与文本嵌入相结合，并将它们输入LLM。本文将LLM注意层中的映射查询和值向量的权值设置为可学习的，以更好地适应具有多个图像的多模态提示。</p>
                            <div style="text-align: center"><img src="../images\VLM\8-6.png" alt="" style="zoom:80%"></div>
                            <p>MMICL一共有两个基于不同LLM的版本，分别基于Vicuna和FlanT5XL两种核心模型。这两个版本都已经开源，其中，FlanT5XL版可以商用，Vicuna版本只能用于科研用途。在MME的多项任务测试中，FlanT5XL版MMICL的成绩已连续数周保持着领先地位。其中认知方面取得了428.93的总成绩（满分800），位列第一，大幅超过了其他模型。感知方面的总分1381.78（满分2000），在最新版榜单中仅次于阿里的千问-7B和昆仑万维的天工模型。所需配置方面，官方给出的说法是在训练阶段需要6块A40，推理阶段则可以在一块A40上运行。仅仅只需要从开源数据集中构建的0.5M的数据即可完成第二阶段的训练，耗时仅需几十小时。</p>
                            <p style="font-weight: bold">会看视频，还能“现学现卖”</p>
                            <p>MMICL支持文本和图片穿插形式的prompt，用起来就像微信聊天一样自然。用正常说话的方式把两张图喂给MMICL，就可以分析出它们的相似和不同之处。除了超强的图像分析能力，MMICL还知道“现学现卖”。比如我们丢给MMICL一张“我的世界”中像素风格的马。由于训练数据都是真实世界的场景，这种过于抽象的像素风MMICL并不认识。但我们只要让MMICL学习几个例子，它便能很快地进行<b>类比推理</b>。下图中，MMICL分别学习了有马、驴和什么都没有这三种场景，然后便正确判断出了更换背景后的像素马。</p>
                            <div style="text-align: center"><img src="../images\VLM\8-1.png" alt="" style="zoom:80%"></div>
                            <p>除了图片，动态的视频也难不倒MMICL，不仅是理解每一帧的内容，还能准确地分析出时空关系，并且，针对视频当中的细节，也可以向MMICL提问。除了准确把握视频中的时空关系，MMICL还支持实时视频流输入。</p>
                            <h5>8.2 模型架构</h5>
                            <p>MMICL致力于解决视觉语言模型在理解具有多个图像的复杂多模态输入方面遇到的问题。MMICL利用Flan-T5 XXL模型作为骨干，整个模型的结构和流程如下图所示：</p>
                            <div style="text-align: center"><img src="../images\VLM\8-2.png" alt="" style="zoom:80%"></div>

                            <p>MMICL使用类似于BLIP2的结构，但是能够接受交错的图文的输入。MMICL将图文平等对待，把处理后的图文特征，都按照输入的格式，拼接成图文交错的形式输入到语言模型中进行训练和推理。类似于InstructBLIP，MMICL的开发过程是将LLM冻结，训练Q-former，并在特定数据集上对其进行微调。MMICL的训练流程和数据构造如下图所示：</p>
                            <div style="text-align: center"><img src="../images\VLM\8-4.png" alt="" style="zoom:80%"></div>
                            <h5>8.3 Context Scheme for MMICL</h5>
                            <p>作者提出了一种新的上下文方案，其中包含一个额外的图像声明部分，并包含图像代理标记，增强了VLM的ICL能力。</p>
                            <div style="text-align: center"><img src="../images\VLM\8-3.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（1）Image declaration</p>
                            <p>用户可以使用文本描述来在其查询中引用特定的图像。这样的参考可以向VLM提供关于文本中提到的视觉内容的信息，允许它学习两种模式之间的对齐。为了精确地链接文本和图像，在混合输入中为每个图像形成image declaration模板，如图3.a所示。首先，分配了一个唯一的图像代理（[IMG$_j$]）来引用图像j的视觉嵌入，它为vlm提供了一个唯一的标识符，以索引和区分视觉嵌入和文本嵌入。然后，利用自然语言提示在文本和图像之间建立引用。在image declaration中包含显式的文本到图像的引用，有助于模型将文本与适当的图像关联起来。同时，作为文本内容进行维护的image declaration，也可以保持显示在提示中的任何位置上的灵活性。每个实例$I_i$都遵循该结构，其中$X_i$象征着可以放置在实例$I_i$中的任何地方的图像装饰集。$q_i$和$a_i$分别表示带有指示的问题和相应的答案.</p>
                            <p>$$I_i=(X_i,q_i,a_i)$$</p>
                            <p>例子：图文间建立的显式指代，MIC在图文交错的数据中，插入图片声明（image declaration），使用图片代理（image proxy）token来代理不同的图片，利用自然语言来建立图文间的指代关系。</p>
                            <div style="text-align: center"><img src="../images\VLM\8-7.png" alt="" style="zoom:80%"></div>

                            <p style="font-weight: bold">（2）Multi-modal data with interconnected images</p>
                            <p>为了在MMICL的上下文模式中包含丰富的多图像信息，作者生成了包括空间、逻辑和时间关系的互联多图像数据。它有助于MMICL理解用户查询中图像之间的复杂关系。具体来说，从视频中推导出帧来构建多图像数据。从视频中提取的帧本质上维持着密切的时间和空间关系，这将图像之间的空间和时间相关信息注入到上下文方案中。此外，我们还从描述多个对象交互的图像中构建多图像数据。我们检测图像中的对象，并为每个对象生成边界框。我们通过根据边界框裁剪图像来获得不同对象的多个子图像。然后，我们将这些对象对应的裁剪图像替换为它们的文本引用，从而用逻辑和因果互联的图像形成交错的多模态数据，如图3.b所示。每个实例$I_i$由一个问答文本对和K个图像组成，其中$x_{i,k} \in X_i$表示第k个图像的图像声明。</p>
                            <p>$$I_i=(\{x_1,x_2,...,x_k\},q_i,a_i)$$</p>
                            <p>例子：时间或逻辑上互相关联的多图数据集，确保了MMICL模型能对图像间的关系有更准确的理解。</p>
                            <div style="text-align: center"><img src="../images\VLM\8-8.png" alt="" style="zoom:80%"></div>

                            <p style="font-weight: bold">（3）Unified multi-modal in-context format for different tasks</p>
                            <p>本文提出了一种针对不同任务生成多模态上下文学习数据的设计方法，以丰富MMICL的上下文方案。它旨在提高VLM的教学感知能力，并扩大其熟练的多模式上下文学习能力。具体来说，首先为每个任务制作不同的指令，并利用这些指令为任务生成不同的模板。然后，用原始任务填充随机选择的模板，以组装配备指令的数据。此外，通过构建从数据中采样实例生成的few-shot样本，将数据转换为多模态的上下文格式。这些示例与输入实例相结合，以生成多模态的上下文内数据。通过这种方式，可以将所有任务转换为统一的多模态上下文格式，如图3.c所示。这种方法促进了收集来自不同任务的大量高质量数据，丰富了MMICL的上下文模式和丰富的多模式上下文数据的多样性。最终，这提高了模型遵循指令的能力和多模态的上下文学习能力。每个实例$I_i$由N个样本组成。</p>
                            <p>$$I_i=(\{P_1,P_2,...,P_N\},q_i,a_i)$$</p>
                            <p>每个范例$P_j=(X_j,q_j,a_j)$，$X_j$表示第j个范例的图像声明。$q_j$和$a_j$分别表示第j个样本的问题和答案。</p>
                            <p>例子：类似于让MMICL“现场学习”的过程，使用多模态的上下文学习来增强MMICL对图文穿插式的复杂图文输入的理解。</p>
                            <div style="text-align: center"><img src="../images\VLM\8-9.png" alt="" style="zoom:80%"></div>

                            <h5>8.4 MIC Dataset Construction</h5>
                            <p>为了帮助vlm理解复杂的提示，作者通过从公共数据资源中收集数据并根据上下文方案进行转换来构建MIC数据集。它有三个关键方面，分别对应8.3节内容的三个方面。MIC的训练集来自8个类别的16个数据集，而测试集来自10个类别的18个数据集。</p>
                            <div style="text-align: center"><img src="../images\VLM\8-5.png" alt="" style="zoom:80%"></div>

                            <h5>8.5 Training</h5>
                            <p>具体来说，MMICL的训练一共分成了两个阶段：</p>
                            <p>- 预训练阶段，使用了LAION-400M（参考LLaVA）数据集</p>
                            <p>- 多模态in-context tuning，使用了自有的MIC（Multi-Model In-Context Learning）数据集</p>

                            <h5>8.6 Discussion</h5>
                            <p>MMICL在多个测试数据集上取得的成绩超过了同样使用FlanT5XXL的BLIP2和InstructionBLIP。尤其是对于涉及多张图的任务，对这种复杂图文输入，MMICL表现了极大的提升。研究团队认为，MMICL解决了视觉语言模型中常常存在的语言偏见（language bais）问题是取得优异成绩的原因之一。大多数视觉语言模型在面对大量文本的上下文内容时会忽视视觉内容，而这是回答需要视觉信息的问题时的致命缺陷。而得益于研究团队的方法，MMICL成功缓解了在视觉语言模型中的这种语言偏见。</p>

                            <h3 id="section9" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://arxiv.org/pdf/2305.03701.pdf">9. LMEye</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: http://model.hitwds.cn:7080/  </li>
                                <li style="list-style-type: circle">Time: 2023.10.16</li>
                                <li style="list-style-type: circle">Company: Harbin Institute of Technology</li>
                                <li style="list-style-type: circle">Parameters: 4.4B</li>
                                <li style="list-style-type: circle">Language Model: FLANT5-XL</li>
                                <li style="list-style-type: circle">Vision Model: CLIP ViT-L/14</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/YunxinLi/LingCloud
                                </li>
                                <li style="list-style-type: circle">Paper: https://arxiv.org/pdf/2305.03701.pdf</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/jUa77CtKsOew0MVQWxaR7g
                                </li>
                            </ul>
                            <h5>9.1 简介</h5>
                            <p>像GPT-4一样，从头开始训练大型视觉语言模型（LVLM）是资源密集型的。本论文提出了一种称为LMEye的方法，这是一种用于大型语言模型（LLM）的交互式感知网络，旨在提高LVLM的图像理解准确性。先前将视觉信息注入LLM的方法利用静态视觉映射网络，但LLM和视觉信息之间缺乏动态交互。<b>（MMICL已经好像解决了？）</b></p>
                            <p>LMEye通过允许LLM结合与人类指令一致的视觉信息来解决这个问题。具体而言，LMEye网络由静态视觉映射网络组成，以向LLM提供图像的基本感知。然后，它还包含额外的线性层，分别负责从LLM获取请求、分解图像特征以及将交织信息传输到LLM。通过这种方式，LLM负责理解人类指令，将其发送到交互式感知网络，并基于交织的多模式信息生成响应。作者通过对多模态问答和推理任务的大量实验来评估LMEye，证明与以前的方法相比，它显著提高了LLM在多模态任务上的零样本性能。</p>
                            <p>主要包括以下贡献：</p>
                            <ul>
                                <li style="list-style-type: disc">为了促进LLM与视觉信息的交互，提出了一个交互式感知工作流程。它总共由四个可学习的线性层组成：</li>
                                <li style="list-style-type: circle;margin-left: 40px">第一层为LLM提供图像的基本全局信息；</li>
                                <li style="list-style-type: circle;margin-left: 40px">另一个主要负责获取LLM理解的人工指令，称为LLM的请求；</li>
                                <li style="list-style-type: circle;margin-left: 40px"> 一种用于将全局图像特征分解为细粒度信息，用于进行请求视觉信息交互；</li>
                                <li style="list-style-type: circle;margin-left: 40px">最后一个负责向LLM发送最终的多模式交互信息。</li>
                                <li style="list-style-type: disc">对于3）和4）之间的交互过程，我们采用冻结文本编码器，通过前缀调优的方式进行多模式信息交互。文本编码器和图像编码器都来自CLIP，并且它们具有相同的表示空间。</li>
                                <li style="list-style-type: disc">为了使LMEye有效，整个训练过程包括：第一个预训练阶段允许LLM获得图像的基本信息，就像BLIP-2中的Q-former一样；调整阶段之后的第二个多模态指令主要使整个交互式感知网络有效工作，并适应各种人类指令。</li>
                            </ul>
                            <h5>9.2 模型架构</h5>
                            <p>特征对齐。我们使用线性投影层将全局图像特征转换到语言嵌入空间。我们不是在特定数据上优化LLM的参数，而是在LLM之外执行人工指令和视觉信息交互。通过这样做，LLM仍然可以保持其原有的权力。</p>
                            <p>交互式感知网络的整体架构示意图。视觉编码器和文本编码器来自CLIP。黑线表示LLM的第一个过程输入，橙色线表示人工查询和视觉信息交互阶段。交互感知网络的整体架构由四个线性层组成，它们是对不同功能的响应。</p>
                            <div style="text-align: center"><img src="../images\VLM\9-1.png" alt="" style="zoom:80%"></div>
                            <h5>9.3 训练</h5>
                            <p><b>多式联运预培训。</b>此阶段旨在训练特征对齐中的线性层。由于CC3M的一些图像没有下载，并且由于字幕质量较差，只使用LAION-400M的部分图像，因此图像-文本对的总数约为6900万。本阶段的优化目标如下：</p>
                            <div style="text-align: center"><img src="../images\VLM\9-2.png" alt="" style="zoom: 60%"></div>
                            <p><b>调整后的多模式指令。</b>这一阶段主要通过使用各种多模式的指令跟随数据，使整个交互式感知网络有效。首先，基于来自数据集CC3M、COCO Caption和Flick3k的图像-文本对构建了两种类型的文本-图像对齐数据，以进一步预训练交互式感知网络。</p>
                            <div style="text-align: center"><img src="../images\VLM\9-3.png" alt="" style="zoom:80%"></div>

                            <h3 id="section10" style="color: coral;margin-top: 50px;font-weight: bold"><a
                                    href="https://github.com/QwenLM/Qwen-VL">10. Qwen-VL-Chat</a>
                            </h3>
                            <ul style="padding-left: 50px">
                                <li style="list-style-type: circle">Demo: <a href="https://modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary">modelscope.cn/studios/qwen/Qwen-VL-Chat-Demo/summary</a>  </li>
                                <li style="list-style-type: circle">Time: 2023.8.24</li>
                                <li style="list-style-type: circle">Company: Alibaba Group</li>
                                <li style="list-style-type: circle">Parameters: 9.6B</li>
                                <li style="list-style-type: circle">Language Model: Qwen-7B</li>
                                <li style="list-style-type: circle">Vision Model: ViT-G/16</li>
                                <li style="list-style-type: circle">GitHub:
                                   https://github.com/QwenLM/Qwen-VL
                                </li>
                                <li style="list-style-type: circle">Paper: https://arxiv.org/pdf/2308.12966.pdf</li>
                                <li style="list-style-type: circle">Reference:
                                    https://mp.weixin.qq.com/s/CRRveurpmbjG7lSY4ZVYYQ，https://mp.weixin.qq.com/s/7s45cz_7G6V8utyBnAi6FQ
                                </li>
                            </ul>
                            <h5>10.1 简介</h5>
                            <p>Qwen 是一个全能的语言模型系列，包含各种参数量的模型，如 Qwen（基础预训练语言模型，即基座模型）和 Qwen-Chat（聊天模型，该模型采用人类对齐技术进行微调）。</p>
                            <ul>
                                <li style="list-style-type: disc"><b>Qwen-VL:</b>以Qwen-7B的预训练模型为语言模型的基础，OpenclipViT-bigG为视觉编码器的初始化，中间加入单层随机初始化的cross-attention，经过约1.5B的图文数据进行训练，使得图像输入分辨率扩大至448。</li>
                                <li style="list-style-type: disc"><b>Qwen-VL-Chat:</b>在Qwen-VL的基础上，使用对齐机制打造了基于大语言模型的视觉AI助手Qwen-VL-Chat。训练数据涵盖了QWen-7B的纯文本SFT数据、开源LVLM的SFT数据、数据合成和人工标注的图文对齐数据。</li>
                            </ul>
                            <p>基座模型在众多下游任务中始终表现出卓越的性能，而聊天模型，尤其是使用人类反馈强化学习（RLHF）训练的模型，具有很强的竞争力。聊天模型Qwen-Chat拥有先进的工具使用和规划能力，可用于创建agent应用程序。即使在使用代码解释器等复杂任务上，Qwen-Chat与更大的模型相比也能表现出极具竞争力的性能。此外，官方还开发了编码专用模型 Code-Qwen 和 Code-Qwen-Chat，以及基于基座模型开发的数学专用模型 Math-Qwen-Chat。与开源模型相比，这些模型的性能有了明显提高，略微落后于专有模型。但是 Code-Qwen和Math-Qwen-Chat都没有开源。</p>
                            <h5>10.2 模型架构</h5>
                            <p>Qwen-VL的整体网络架构由三个部分组成，模型参数详情如Table 1所示。</p>
                            <div style="text-align: center"><img src="../images\VLM\10-1.png" alt=""></div>
                            <ul>
                                <li style="list-style-type: disc"><b>语言大模型:</b>Qwen-VL采用大型语言模型作为基础组件。该模型使用来自Qwen- 7b 的预训练权值进行初始化。</li>
                                <li style="list-style-type: disc"><b>视觉编码器:</b>Qwen-VL的视觉编码器使用Vision Transformer (ViT)架构，使用Openclip的ViT- bigG 预训练的权重进行初始化。在训练和推理过程中，输入图像被调整到特定的分辨率。视觉编码器通过将图像分成14步的小块来处理图像，生成一组图像特征。</li>
                                <li style="list-style-type: disc"><b>位置感知视觉语言适配器:</b>为了缓解长图像特征序列带来的效率问题，Qwen-VL引入了一个压缩图像特征的视觉语言适配器。该适配器包括一个随机初始化的单层交叉注意模块。该模块使用一组可训练向量(Embeddings)作为查询向量，并使用视觉编码器的图像特征作为交叉注意操作的key。该机制将视觉特征序列压缩为256的固定长度。此外，考虑到位置信息对细粒度图像理解的重要性，将二维绝对位置编码纳入交叉注意机制的query-key对中，以减轻压缩过程中可能丢失的位置细节。随后将长度为256的压缩图像特征序列输入到大型语言模型中。</li>
                            </ul>
                            <h5>10.3 输入和输出</h5>
                            <ul>
                                <li style="list-style-type: disc"><b>图像输入:</b>通过视觉编码器和适配器对图像进行处理，产生固定长度的图像特征序列。为了区分图像特征输入和文本特征输入，在图像特征序列的开头和结尾分别添加两个特殊的token(&lt;img&gt;和&lt;/img&gt;)，表示图像内容的开始和结束。</li>
                                <li style="list-style-type: disc"><b>边界框输入和输出:</b>为了增强模型对细粒度视觉理解和基础的能力，Qwen-VL的训练涉及区域描述、问题和检测形式的数据。与涉及图像-文本描述或问题的传统任务不同，该任务需要模型准确理解并以指定格式生成区域描述。对于任何给定的边界框，应用normalization过程(在[0,1000]范围内)并将其转换为指定的字符串格式:“(Xtoplef t, Ytoplef t)，(Xbottomright, Ybottomright)”。字符串被标记为文本，不需要额外的位置词汇表。为了区分检测字符串和常规文本字符串，需要使用两个特殊标记(&lt;box&gt;和 &lt;/box&gt;在边界框字符串的开始和结束处添加。此外，为了适当地将边界框与其相应的描述性单词或句子关联起来，另一组特殊的标记(&lt;ref&gt;  和&lt;/ref&gt;)被引入标记边界框引用的内容。</li>
                            </ul>
                            <h5>10.4 Training</h5>
                            <p>Qwen-VL模型的训练过程包括三个阶段:两个阶段的预训练和最后一个阶段的指令微调训练。</p>
                            <div style="text-align: center"><img src="../images\VLM\10-2.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（1）Pretraining</p>
                            <p>在预训练的第一阶段主要利用一个大规模的、弱标记的、网络抓取的图像-文本对集合。预训练数据集由几个可公开访问的数据源和一些内部数据组成。如下表所示，原始数据集共包含50亿对图像-文本对，清洗后还剩下14亿对数据，其中英文(文本)数据占77.3%，中文(文本)数据占22.7%。预训练第一阶段训练数据如表2所示：</p>
                            <div style="text-align: center"><img src="../images\VLM\10-3.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（2）Multi-task Pre-training</p>
                            <p>在多任务预训练的第二阶段，<b>引入了高质量和细粒度的VL标注数据，具有更大的输入分辨率和交错的图像-文本数据</b>。
                                如下表所示，同时对Qwen-VL进行了7项任务的训练。对于文本生成，使用内部收集的语料库来保持LLM的能力。
                                这一阶段<b>将视觉编码器的输入分辨率从224 × 224提高到448 × 448，减少了图像降采样带来的信息损失。</b>
                                解锁了大的语言模型并训练了整个模型。训练目标与预训练第一阶段相同。</p>
                            <div style="text-align: center"><img src="../images\VLM\10-4.png" alt="" style="zoom:80%"></div>
                            <p style="font-weight: bold">（3）Supervised Fine-tuning</p>
                            <p>在这个阶段主要<b>通过指令微调对Qwen-VL预训练模型进行微调，增强其指令跟随和对话能力，从而形成交互式Qwen-VL- chat模型</b>。
                                多模态指令调优数据主要来自于通过LLM自指令生成的标题数据或对话数据，通常只解决单图像对话和推理，局限于图像内容理解。
                                通过手工标注、模型生成和策略连接构建了一组额外的对话数据，以将定位和多图像理解能力纳入Qwen-VL模型。该模型有效地将这些功能转移到更广泛的语言和问题类型中。
                                此外，在训练过程中混合了多模态和纯文本对话数据，以确保模型在对话能力方面的通用性。指令调优数据总量为350k。</p>
                            <p>在训练期间，<b>通过只监督答案和特殊tokens(示例中的蓝色)来确保预测和训练分布之间的一致性，而不监督角色名称或问题提示</b>。
                                在此阶段，冻结了视觉编码器，优化了语言模型和适配器模块。具体来说，Qwen-VL-Chat的全局批大小为128，学习率计划的最大学习率为1e−5，
                                最小学习率为1e−6，线性热身为3000步。</p>
                            <div style="text-align: center"><img src="../images\VLM\10-5.png" alt="" style="zoom:80%"></div>
                            <h5>10.5 Discussion</h5>
                            <p>研究人员在四大类多模态任务（Zero-shot Caption/VQA/DocVQA/Grounding）的标准英文测评中测试了Qwen-VL。结果显示，Qwen-VL取得了同等尺寸开源LVLM的最好效果。另外，研究人员构建了一套基于GPT-4打分机制的测试集<b>TouchStone</b>。在这一对比测试中，Qwen-VL-Chat取得了SOTA。</p>
                        </div>
			</div>
		</div>
	</div>
		</div>
</main>
<script src="../js/jquery-1.11.0.min.js"></script>
<script src="https://cdn.bootcdn.net/ajax/libs/twitter-bootstrap/5.3.0-alpha3/js/bootstrap.bundle.min.js" integrity="sha384-ENjdO4Dr2bkBIFxQpeoTz1HIcje39Wm4jDKdf19U8gI4ddQ3GYNS7NTKfAdVQSZe" crossorigin="anonymous"></script>
</body>
</html>